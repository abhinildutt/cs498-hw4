-----app.py-----
from flask import Flask, request, jsonify
import json
import os
import glob
from collections import OrderedDict

app = Flask(__name__)

# Load processed data from JSON files
def load_data():
    data = []
    json_files = glob.glob("processed_data/part-*.json")
    
    for file_path in json_files:
        with open(file_path, 'r') as file:
            for line in file:
                try:
                    record = json.loads(line)
                    data.append(record)
                except json.JSONDecodeError:
                    continue
    
    return data

# Global data variable to avoid reloading for every request
processed_data = None

@app.before_first_request
def initialize():
    global processed_data
    processed_data = load_data()

@app.route('/results', methods=['POST'])
def get_results():
    global processed_data
    if processed_data is None:
        processed_data = load_data()
    
    request_data = request.get_json()
    search_term = request_data.get('term')
    
    if not search_term:
        return jsonify({"error": "Search term is required"}), 400
    
    # Filter records by search term, stripping quotes from stored terms
    results = {}
    term_records = [r for r in processed_data if r['term'][1:-1] == search_term]
    
    for record in term_records:
        url = record['url']
        
        # Ensure clicks is an integer and handle whitespace if it's a string
        clicks = record['clicks']
        if isinstance(clicks, str):
            clicks = int(clicks.strip())
        else:
            clicks = int(clicks)
            
        results[url] = clicks

    # Sort results by click count in descending order and then by domain priority
    def get_domain_priority(url):
        if ".org" in url:
            return 0
        elif ".edu" in url:
            return 1
        elif ".com" in url:
            return 2
        else:
            return 3
    
    # Sort by clicks (descending) first, then by domain priority
    sorted_results = sorted(results.items(), 
                           key=lambda item: (-item[1], get_domain_priority(item[0])))
    
    # Convert back to dictionary (using dict to preserve insertion order in Python 3.7+)
    sorted_results_dict = dict(sorted_results)

    return jsonify({"results": sorted_results_dict})

@app.route('/trends', methods=['POST'])
def get_trends():
    global processed_data
    if processed_data is None:
        processed_data = load_data()
    
    request_data = request.get_json()
    search_term = request_data.get('term')
    
    if not search_term:
        return jsonify({"error": "Search term is required"}), 400
    
    # Calculate total clicks for the search term, stripping quotes from stored terms
    total_clicks = sum(r['clicks'] for r in processed_data if r['term'][1:-1] == search_term)
    
    return jsonify({"clicks": total_clicks})

@app.route('/popularity', methods=['POST'])
def get_popularity():
    global processed_data
    if processed_data is None:
        processed_data = load_data()
    
    request_data = request.get_json()
    url = request_data.get('url')
    
    if not url:
        return jsonify({"error": "URL is required"}), 400
    
    # Calculate total clicks for the URL
    total_clicks = sum(r['clicks'] for r in processed_data if r['url'] == url)
    
    return jsonify({"clicks": total_clicks})

@app.route('/getBestTerms', methods=['POST'])
def get_best_terms():
    global processed_data
    if processed_data is None:
        processed_data = load_data()
    
    request_data = request.get_json()
    website = request_data.get('website')
    
    if not website:
        return jsonify({"error": "Website URL is required"}), 400
    
    # Calculate total clicks for the website
    website_records = [r for r in processed_data if r['url'] == website]
    total_website_clicks = sum(r['clicks'] for r in website_records)
    
    if total_website_clicks == 0:
        return jsonify({"best_terms": []})
    
    # Find terms where website received more than 5% of total clicks
    best_terms = []
    url_term_clicks = {}
    
    # Group clicks by term for this URL
    for record in website_records:
        # Strip quotes from the term
        term = record['term'][1:-1]
        clicks = record['clicks']
        if term in url_term_clicks:
            url_term_clicks[term] += clicks
        else:
            url_term_clicks[term] = clicks
    
    # Check which terms meet the 5% threshold
    for term, clicks in url_term_clicks.items():
        if clicks / total_website_clicks > 0.05:
            best_terms.append(term)
    
    return jsonify({"best_terms": best_terms})

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=3000, debug=True) 


-----------------
-----preprocess.py-------

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("SearchLogProcessor") \
    .getOrCreate()

search_log_files = spark.sparkContext.wholeTextFiles("searchLog.csv")
search_log_content = search_log_files.map(lambda x: x[1])

def split_into_logical_lines(content):
    cleaned_content = content.replace("\n", " ")
    parts = cleaned_content.split("searchTerm:")
    logical_lines = ["searchTerm:" + part.strip() for part in parts[1:] if part.strip()]
    return logical_lines

# Get logical lines containing complete search terms
logical_lines = search_log_content.flatMap(split_into_logical_lines)

# Function to parse each logical line
def parse_line(line):
    results = []
    line = line.strip()
    if not line.startswith("searchTerm:"):
        return results
    
    content = line[len("searchTerm:"):].strip()
    
    parts = content.split(",", 1)
    if len(parts) != 2:
        return results
    
    term = parts[0].strip()
    url_clicks_part = parts[1].strip()
    
    url_clicks = url_clicks_part.split("~")
    
    for url_click in url_clicks:
        url_click = url_click.strip()
        if ":" in url_click:
            url_parts = url_click.split(":", 1)
            if len(url_parts) == 2:
                url, clicks = url_parts
                url = url.strip()
                try:
                    clicks = int(clicks.strip())
                    results.append((term, url, clicks))
                except ValueError:
                    continue
    
    return results

# Parse the log file and extract records
records = logical_lines.flatMap(parse_line)

# Define schema for the DataFrame
schema = StructType([
    StructField("term", StringType(), True),
    StructField("url", StringType(), True),
    StructField("clicks", IntegerType(), True)
])

# Create DataFrame from RDD
df = spark.createDataFrame(records, schema)

# Sort the DataFrame to ensure consistent ordering
# First by term, then by clicks in descending order, then by url
df = df.orderBy("term", df.clicks.desc(), "url")

# Save DataFrame as JSON
df.write.mode("overwrite").json("processed_data")

# Stop the Spark session
spark.stop()

---------

data image 

